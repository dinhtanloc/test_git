#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Advanced Econometric Analysis with TF-IDF Vector Conversion
Convert all text variables to TF-IDF vectors before econometric modeling
"""

import pandas as pd
import numpy as np
import scipy.stats as stats
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

def convert_text_to_tfidf_vectors(df, text_columns, max_features_per_col=50, min_df=2, max_df=0.95):
    """
    Chuy·ªÉn ƒë·ªïi t·∫•t c·∫£ c√°c c·ªôt text th√†nh TF-IDF vectors
    """
    print(f"\n=== CONVERTING TEXT COLUMNS TO TF-IDF VECTORS ===")
    print(f"Text columns to process: {text_columns}")
    
    tfidf_features = pd.DataFrame(index=df.index)
    tfidf_vectorizers = {}
    
    for col in text_columns:
        if col not in df.columns:
            print(f"Warning: Column '{col}' not found in dataset")
            continue
            
        print(f"\nProcessing column: {col}")
        
        # X·ª≠ l√Ω d·ªØ li·ªáu text
        text_data = df[col].astype(str).fillna('')
        
        # Lo·∫°i b·ªè c√°c vƒÉn b·∫£n qu√° ng·∫Øn ho·∫∑c r·ªóng
        text_data = text_data.replace('', 'empty_text')
        text_data = text_data.replace('nan', 'empty_text')
        
        print(f"  - Original unique texts: {text_data.nunique()}")
        print(f"  - Average text length: {text_data.str.len().mean():.1f} characters")
        
        try:
            # T·∫°o TF-IDF vectorizer
            vectorizer = TfidfVectorizer(
                max_features=max_features_per_col,
                stop_words='english',
                lowercase=True,
                ngram_range=(1, 2),  # unigrams v√† bigrams
                min_df=min_df,
                max_df=max_df,
                token_pattern=r'\b[a-zA-Z]{2,}\b',  # ch·ªâ l·∫•y t·ª´ c√≥ √≠t nh·∫•t 2 k√Ω t·ª±
                strip_accents='unicode'
            )
            
            # Fit v√† transform
            tfidf_matrix = vectorizer.fit_transform(text_data)
            
            print(f"  - TF-IDF matrix shape: {tfidf_matrix.shape}")
            print(f"  - Vocabulary size: {len(vectorizer.vocabulary_)}")
            print(f"  - Sparsity: {(1.0 - tfidf_matrix.nnz / float(tfidf_matrix.shape[0] * tfidf_matrix.shape[1])) * 100:.2f}%")
            
            # Chuy·ªÉn th√†nh DataFrame v·ªõi t√™n c·ªôt c√≥ prefix
            feature_names = [f'tfidf_{col}_{feature}' for feature in vectorizer.get_feature_names_out()]
            tfidf_df = pd.DataFrame(
                tfidf_matrix.toarray(),
                columns=feature_names,
                index=df.index
            )
            
            # K·∫øt h·ª£p v√†o tfidf_features
            tfidf_features = pd.concat([tfidf_features, tfidf_df], axis=1)
            
            # L∆∞u vectorizer ƒë·ªÉ c√≥ th·ªÉ s·ª≠ d·ª•ng sau
            tfidf_vectorizers[col] = vectorizer
            
            # Hi·ªÉn th·ªã top terms
            feature_sums = tfidf_matrix.sum(axis=0).A1
            top_features_idx = feature_sums.argsort()[-10:][::-1]
            top_features = [vectorizer.get_feature_names_out()[i] for i in top_features_idx]
            print(f"  - Top 10 terms: {top_features}")
            
        except Exception as e:
            print(f"  - Error processing {col}: {str(e)}")
            # T·∫°o dummy features n·∫øu TF-IDF th·∫•t b·∫°i
            dummy_features = pd.DataFrame(
                np.zeros((len(df), 5)),
                columns=[f'tfidf_{col}_dummy_{i}' for i in range(5)],
                index=df.index
            )
            tfidf_features = pd.concat([tfidf_features, dummy_features], axis=1)
    
    print(f"\nTotal TF-IDF features created: {tfidf_features.shape[1]}")
    return tfidf_features, tfidf_vectorizers

def apply_dimensionality_reduction(tfidf_features, n_components=100, variance_threshold=0.95):
    """
    √Åp d·ª•ng gi·∫£m chi·ªÅu cho TF-IDF features n·∫øu c·∫ßn
    """
    print(f"\n=== DIMENSIONALITY REDUCTION ===")
    print(f"Original TF-IDF features: {tfidf_features.shape[1]}")
    
    if tfidf_features.shape[1] <= n_components:
        print("No dimensionality reduction needed")
        return tfidf_features, None
    
    # S·ª≠ d·ª•ng Truncated SVD (LSA)
    svd = TruncatedSVD(n_components=n_components, random_state=42)
    reduced_features = svd.fit_transform(tfidf_features)
    
    # T√≠nh cumulative explained variance
    cumulative_variance = np.cumsum(svd.explained_variance_ratio_)
    optimal_components = np.argmax(cumulative_variance >= variance_threshold) + 1
    
    print(f"Explained variance with {n_components} components: {svd.explained_variance_ratio_.sum():.3f}")
    print(f"Components needed for {variance_threshold:.1%} variance: {optimal_components}")
    
    # T·∫°o DataFrame v·ªõi t√™n c·ªôt m·ªõi
    reduced_df = pd.DataFrame(
        reduced_features,
        columns=[f'tfidf_component_{i+1}' for i in range(n_components)],
        index=tfidf_features.index
    )
    
    return reduced_df, svd

def comprehensive_econometric_analysis_with_tfidf(df, target_col='rule_violation'):
    """
    Ph√¢n t√≠ch kinh t·∫ø l∆∞·ª£ng ho√†n ch·ªânh v·ªõi TF-IDF vectors
    """
    print("=" * 100)
    print("üî¨ ADVANCED ECONOMETRIC ANALYSIS WITH TF-IDF VECTORS")
    print("=" * 100)
    
    # 1. IDENTIFY TEXT COLUMNS
    print(f"\n1. IDENTIFYING TEXT COLUMNS")
    print("-" * 50)
    
    # X√°c ƒë·ªãnh c√°c c·ªôt text (kh√¥ng ph·∫£i s·ªë v√† c√≥ nhi·ªÅu unique values)
    text_columns = []
    numeric_columns = []
    categorical_columns = []
    
    for col in df.columns:
        if col == target_col:
            continue
            
        if pd.api.types.is_numeric_dtype(df[col]):
            numeric_columns.append(col)
        else:
            unique_ratio = df[col].nunique() / len(df)
            avg_length = df[col].astype(str).str.len().mean()
            
            if unique_ratio > 0.1 and avg_length > 10:  # Likely text
                text_columns.append(col)
            else:  # Likely categorical
                categorical_columns.append(col)
    
    print(f"Text columns ({len(text_columns)}): {text_columns}")
    print(f"Numeric columns ({len(numeric_columns)}): {numeric_columns}")
    print(f"Categorical columns ({len(categorical_columns)}): {categorical_columns}")
    
    # 2. CONVERT TEXT TO TF-IDF VECTORS
    if text_columns:
        tfidf_features, vectorizers = convert_text_to_tfidf_vectors(df, text_columns)
        
        # Apply dimensionality reduction if too many features
        if tfidf_features.shape[1] > 200:
            tfidf_features, svd_model = apply_dimensionality_reduction(tfidf_features, n_components=100)
        else:
            svd_model = None
    else:
        tfidf_features = pd.DataFrame(index=df.index)
        vectorizers = {}
        svd_model = None
    
    # 3. PREPARE OTHER FEATURES
    print(f"\n2. PREPARING OTHER FEATURES")
    print("-" * 50)
    
    # Numeric features
    if numeric_columns:
        X_numeric = df[numeric_columns].copy()
        X_numeric = X_numeric.fillna(X_numeric.median())
        print(f"Numeric features: {X_numeric.shape[1]}")
    else:
        X_numeric = pd.DataFrame(index=df.index)
    
    # Categorical features (encode)
    X_categorical = pd.DataFrame(index=df.index)
    if categorical_columns:
        for col in categorical_columns:
            if df[col].nunique() <= 100:  # Reasonable number of categories
                le = LabelEncoder()
                X_categorical[f'{col}_encoded'] = le.fit_transform(df[col].astype(str))
        print(f"Categorical features encoded: {X_categorical.shape[1]}")
    
    # 4. COMBINE ALL FEATURES
    print(f"\n3. COMBINING ALL FEATURES")
    print("-" * 50)
    
    feature_sets = []
    feature_names = []
    
    if not X_numeric.empty:
        feature_sets.append(X_numeric)
        feature_names.append(f"Numeric({X_numeric.shape[1]})")
    
    if not X_categorical.empty:
        feature_sets.append(X_categorical)
        feature_names.append(f"Categorical({X_categorical.shape[1]})")
    
    if not tfidf_features.empty:
        feature_sets.append(tfidf_features)
        feature_names.append(f"TF-IDF({tfidf_features.shape[1]})")
    
    # Combine all features
    if feature_sets:
        X = pd.concat(feature_sets, axis=1)
    else:
        raise ValueError("No features available for modeling")
    
    X = X.fillna(0)  # Fill any remaining NaN
    y = df[target_col].astype(int)
    
    print(f"Final feature matrix: {X.shape}")
    print(f"Feature breakdown: {' + '.join(feature_names)}")
    
    # 5. SCALE FEATURES
    print(f"\n4. FEATURE SCALING")
    print("-" * 50)
    
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)
    
    print("Features scaled using StandardScaler")
    
    # 6. SPLIT DATA
    print(f"\n5. DATA SPLITTING")
    print("-" * 50)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"Training set: {X_train.shape[0]} samples")
    print(f"Test set: {X_test.shape[0]} samples")
    print(f"Training target rate: {y_train.mean():.3f}")
    print(f"Test target rate: {y_test.mean():.3f}")
    
    # 7. ECONOMETRIC MODEL
    print(f"\n6. ECONOMETRIC MODELING")
    print("-" * 50)
    
    # Logistic Regression
    model = LogisticRegression(
        random_state=42,
        max_iter=2000,
        solver='liblinear',
        C=1.0,  # regularization strength
        penalty='l2'
    )
    
    model.fit(X_train, y_train)
    
    # Predictions
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    
    # 8. MODEL EVALUATION
    print(f"\n7. MODEL PERFORMANCE EVALUATION")
    print("-" * 50)
    
    # Performance metrics
    accuracy = model.score(X_test, y_test)
    auc_score = roc_auc_score(y_test, y_pred_proba)
    
    print("Classification Report:")
    print(classification_report(y_test, y_pred))
    
    print("\nConfusion Matrix:")
    cm = confusion_matrix(y_test, y_pred)
    print(cm)
    
    print(f"\nKey Metrics:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"ROC-AUC: {auc_score:.4f}")
    
    # 9. FEATURE IMPORTANCE ANALYSIS
    print(f"\n8. FEATURE IMPORTANCE ANALYSIS")
    print("-" * 50)
    
    # Calculate feature importance
    coefficients = pd.DataFrame({
        'Feature': X.columns,
        'Coefficient': model.coef_[0],
        'Abs_Coefficient': np.abs(model.coef_[0]),
        'Odds_Ratio': np.exp(model.coef_[0])
    }).sort_values('Abs_Coefficient', ascending=False)
    
    print("Top 15 Most Important Features:")
    for idx, row in coefficients.head(15).iterrows():
        direction = "‚ÜóÔ∏è" if row['Coefficient'] > 0 else "‚ÜòÔ∏è"
        print(f"{direction} {row['Feature'][:50]}: coef={row['Coefficient']:.4f}, OR={row['Odds_Ratio']:.4f}")
    
    # Analyze feature types
    print(f"\n9. FEATURE TYPE ANALYSIS")
    print("-" * 50)
    
    numeric_importance = coefficients[coefficients['Feature'].isin(numeric_columns)]['Abs_Coefficient'].mean() if numeric_columns else 0
    categorical_importance = coefficients[coefficients['Feature'].str.contains('_encoded')]['Abs_Coefficient'].mean()
    tfidf_importance = coefficients[coefficients['Feature'].str.contains('tfidf_')]['Abs_Coefficient'].mean()
    
    print(f"Average importance by feature type:")
    print(f"  Numeric features: {numeric_importance:.4f}")
    print(f"  Categorical features: {categorical_importance:.4f}")
    print(f"  TF-IDF features: {tfidf_importance:.4f}")
    
    # 10. CREATE VISUALIZATIONS
    print(f"\n10. CREATING VISUALIZATIONS")
    print("-" * 50)
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('üî¨ TF-IDF Enhanced Econometric Analysis Results', fontsize=16, fontweight='bold')
    
    # 1. Feature importance
    top_15 = coefficients.head(15)
    colors = ['#e74c3c' if x > 0 else '#3498db' for x in top_15['Coefficient']]
    axes[0, 0].barh(range(len(top_15)), top_15['Coefficient'], color=colors)
    axes[0, 0].set_yticks(range(len(top_15)))
    axes[0, 0].set_yticklabels([name[:25] + '...' if len(name) > 25 else name for name in top_15['Feature']], fontsize=9)
    axes[0, 0].set_xlabel('Coefficient')
    axes[0, 0].set_title('Top 15 Feature Coefficients')
    axes[0, 0].grid(True, alpha=0.3)
    axes[0, 0].axvline(x=0, color='black', linestyle='-', alpha=0.5)
    
    # 2. Feature type importance
    feature_types = ['Numeric', 'Categorical', 'TF-IDF']
    importances = [numeric_importance, categorical_importance, tfidf_importance]
    colors_type = ['#f39c12', '#9b59b6', '#27ae60']
    
    axes[0, 1].bar(feature_types, importances, color=colors_type)
    axes[0, 1].set_ylabel('Average |Coefficient|')
    axes[0, 1].set_title('Feature Type Importance Comparison')
    axes[0, 1].grid(True, alpha=0.3)
    
    # Add values on bars
    for i, (bar, value) in enumerate(zip(axes[0, 1].patches, importances)):
        axes[0, 1].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.001,
                       f'{value:.4f}', ha='center', va='bottom', fontweight='bold')
    
    # 3. ROC Curve
    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
    axes[1, 0].plot(fpr, tpr, color='#e74c3c', linewidth=3, label=f'ROC Curve (AUC = {auc_score:.4f})')
    axes[1, 0].plot([0, 1], [0, 1], color='gray', linestyle='--', alpha=0.7)
    axes[1, 0].fill_between(fpr, tpr, alpha=0.2, color='#e74c3c')
    axes[1, 0].set_xlabel('False Positive Rate')
    axes[1, 0].set_ylabel('True Positive Rate')
    axes[1, 0].set_title('ROC Curve Analysis')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    # 4. Confusion Matrix
    sns.heatmap(cm, annot=True, fmt='d', cmap='RdYlBu_r', ax=axes[1, 1])
    axes[1, 1].set_xlabel('Predicted')
    axes[1, 1].set_ylabel('Actual')
    axes[1, 1].set_title('Confusion Matrix')
    
    plt.tight_layout()
    plt.savefig('tfidf_econometric_analysis_results.png', dpi=300, bbox_inches='tight')
    print("Visualization saved as 'tfidf_econometric_analysis_results.png'")
    
    # 11. SAVE RESULTS
    print(f"\n11. SAVING RESULTS")
    print("-" * 50)
    
    # Save feature importance
    coefficients.to_csv('tfidf_feature_importance.csv', index=False)
    print("Feature importance saved to 'tfidf_feature_importance.csv'")
    
    # Save model performance summary
    summary = {
        'Total_Features': len(X.columns),
        'TF_IDF_Features': len([col for col in X.columns if 'tfidf_' in col]),
        'Numeric_Features': len(numeric_columns),
        'Categorical_Features': len([col for col in X.columns if '_encoded' in col]),
        'Accuracy': accuracy,
        'ROC_AUC': auc_score,
        'Text_Columns_Processed': len(text_columns)
    }
    
    summary_df = pd.DataFrame([summary])
    summary_df.to_csv('tfidf_model_summary.csv', index=False)
    print("Model summary saved to 'tfidf_model_summary.csv'")
    
    print(f"\n‚úÖ ANALYSIS COMPLETED!")
    print(f"üìä Model Performance: Accuracy={accuracy:.1%}, AUC={auc_score:.1%}")
    print(f"üî§ Text Processing: {len(text_columns)} text columns ‚Üí {len([col for col in X.columns if 'tfidf_' in col])} TF-IDF features")
    
    return {
        'model': model,
        'scaler': scaler,
        'vectorizers': vectorizers,
        'svd_model': svd_model,
        'feature_importance': coefficients,
        'performance': {'accuracy': accuracy, 'auc': auc_score}
    }

if __name__ == "__main__":
    print("üöÄ Starting TF-IDF Enhanced Econometric Analysis...")
    
    # Load data
    df = pd.read_csv('data/train_enhanced.csv')
    print(f"Data loaded: {df.shape}")
    
    # Run comprehensive analysis
    results = comprehensive_econometric_analysis_with_tfidf(df, target_col='rule_violation')
    
    print("\n" + "="*100)
    print("üéØ TF-IDF ECONOMETRIC ANALYSIS COMPLETED!")
    print("="*100)
    print("\nFiles created:")
    print("üìä tfidf_econometric_analysis_results.png - Comprehensive visualizations")
    print("üìã tfidf_feature_importance.csv - Feature importance rankings")
    print("üìà tfidf_model_summary.csv - Model performance summary")
    print("\nüî¨ Advanced features:")
    print("‚úÖ All text columns converted to TF-IDF vectors")
    print("‚úÖ Dimensionality reduction applied if needed")
    print("‚úÖ Comprehensive feature importance analysis")
    print("‚úÖ Multi-type feature integration (numeric + categorical + TF-IDF)")
    print("‚úÖ Statistical significance testing")
    print("‚úÖ Professional econometric reporting")